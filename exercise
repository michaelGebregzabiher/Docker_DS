7. Conclusion and examination
a. Conclusion
Docker and containers in general are a very fashionable and sought-after technology, whether among Data players or developers more generally. It makes it very easy to develop applications, package them and isolate them from the production server. In a process of moving from development to production, it therefore simplifies and accelerates the testing and deployment phases.

With a very active community and many pre-existing images, official or not, Docker is a major asset for Data Scientists, Machine Learning Engineers and Data Engineers.

b. Exercise
To validate this module, you must complete the following exercise. Please read the instructions carefully.

Presentation
For the correction of this exercise, we will try to create a CI/CD pipeline to test an API. We will put ourselves in the shoes of a team that is supposed to create a set of tests to be applied automatically before deployment.

In our scenario, a team has created an application that allows to use a sentiment analysis algorithm: it allows to predict if a sentence is positive or negative. This API will be deployed in a container whose image is for the moment datascientest/fastapi:1.0.0.

Let's look at the entry points of our API:

/status returns 1 if the API is running
/permissions returns a user's permissions
/v1/sentiment returns the sentiment analysis using an old model
/v2/sentiment returns the sentiment analysis using a new template
The /status entry point simply checks that the API is working. The /permissions entry point allows someone, identified by a username and a password to see which version of the template they have access to. Finally the last two take a sentence as input, check that the user is identified, check that the user has the right to use this template and if so, return the sentiment score: -1 is negative; +1 is positive.

To download the image, run the following command

docker image pull datascientest/fastapi:1.0.0
To test the API manually, run the command

docker container run -p 8000:8000 datascientest/fastapi:1.0.0
The API is available on port 8000 of the host machine. At the entry point /docs you can find a detailed description of the entry points.

We will define some test scenarios that will be done via separate containers.

Tests
Authentication
In this first test, we are going to check that the identification logic works well. To do this, we will need to make GET requests on the /permissions entry point. We know that two users exist alice and bob and their passwords are wonderland and builder. We'll try a 3rd test with a password that doesn't work: clementine and mandarine.

The first two requests should return a 200 error code while the third should return a 403 error code.

Authorization
In this second test, we will verify that our user authorization logic is working properly. We know that bob only has access to v1 while alice has access to both versions. For each of the users, we will make a query on the /v1/sentiment and /v2/sentiment entry points: we must then provide the arguments username, password and sentence which contains the sentence to be analyzed.

Content
In this last test, we check that the API works as it should. We will test the following sentences with the alice account:

life is beautiful
that sucks
For each version of the model, we should get a positive score for the first sentence and a negative score for the second sentence. The test will consist in checking the positivity or negativity of the score.

Building the tests
For each of the tests, we want to create a separate container that will run those tests. The idea of having one container per test means that the entire test pipeline does not have to be changed if only one of the components has changed.

When a test is run, if an environment variable LOG is set to 1, then a log should be printed in a api_test.log file.

You are free to choose the technology used: the Python libraries requests and os seem to be affordable options. As the core of this exercise is not so much Python programming, we propose you a possible code example for a test part :

import os
import requests
# definition of the API address
api_address = ''
# API port
api_port = 8000
# requÃªte
r = requests.get(
    url='http://{address}:{port}/permissions'.format(address=api_address, port=api_port),
    params= {
        'username': 'alice',
        'password': 'wonderland'
    }
)
output = '''
============================
    Authentication test
============================
request done at "/permissions"
| username="alice"
| password="wonderland"
expected result = 200
actual restult = {status_code}
==>  {test_status}
'''
# query status
status_code = r.status_code
# display the results
if status_code == 200:
    test_status = 'SUCCESS'
else:
    test_status = 'FAILURE'
print(output.format(status_code=status_code, test_status=test_status))
# printing in a file
if os.environ.get('LOG') == 1:
    with open('api_test.log', 'a') as file:
        file.write(output)
So we have to realize the code for each of the tests (Authentication, Authorization and Content). Then, we will build Docker images via DockerFile to launch these tests. Think carefully about the arguments you want to pass to the container (command to launch, environment variables, ...). Do not hesitate to test your code directly on the container from an iPython console or even a Jupyter notebook.

Docker Compose
We saw it earlier but Docker Compose is a very used tool for CI/CD pipelines. It allows us to run our different tests all at once while making it easy to share data between different tests. Here you are asked to create a docker-compose.yml file that organizes this pipeline. Think about the use of container names, environment variables and networks.

Docker-Compose will have to launch 4 containers: the API container and the 3 test containers. At the end of the execution of the different tests, we want to have the api_test.log file with the report of all the tests. For this, we can use the volumes wisely.

Rendering
The expectations of this exercise are:

a docker-compose.yml file that contains the sequence of tests to be performed
the Python files used in the Docker images
the Dockerfile used to build these images
a file called setup.sh containing the commands used to build the images and launch the docker-compose
the result of the logs in a log.txt file
a docker-compose.yml file that contains the sequence of tests to be performed
possibly a file of remarks or justification of the choices made
Don't forget to upload your exam as a zip or tar archive, in the My Exams tab, after you have validated all the exercises in the module.

Good luck !
